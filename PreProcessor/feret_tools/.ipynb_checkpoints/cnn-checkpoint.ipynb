{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1edf63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:36.553260662Z",
     "start_time": "2024-01-13T18:45:36.354249321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 17:52:40.267020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 17:52:40.941570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd68aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:38.861097906Z",
     "start_time": "2024-01-13T18:45:38.840973422Z"
    }
   },
   "outputs": [],
   "source": [
    "train_csv_filename = '../merged_feret_train.csv' \n",
    "test_csv_filename = '../merged_feret_test.csv'\n",
    "train_df = pd.read_csv(train_csv_filename)\n",
    "test_df = pd.read_csv(test_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc24e50f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:39.451608808Z",
     "start_time": "2024-01-13T18:45:39.443174560Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac = 1)\n",
    "test_df = test_df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c3beeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:39.867667498Z",
     "start_time": "2024-01-13T18:45:39.854838853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>is_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>00446_940422_fa_converted_final.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>00055_931230_fa_a_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>00575_940928_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>00127_931230_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>00351_940422_fa_converted_final.png</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>00302_940422_fa_converted_final.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>00384_940422_fa_converted_final.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>00318_940422_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>00183_940128_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>00164_931230_fa_a_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  filename  gender  race  is_generated\n",
       "147    00446_940422_fa_converted_final.png       1     1         False\n",
       "82   00055_931230_fa_a_converted_final.png       0     2         False\n",
       "72     00575_940928_fa_converted_final.png       0     3         False\n",
       "119    00127_931230_fa_converted_final.png       0     1         False\n",
       "55     00351_940422_fa_converted_final.png       1     3         False\n",
       "..                                     ...     ...   ...           ...\n",
       "129    00302_940422_fa_converted_final.png       1     1         False\n",
       "139    00384_940422_fa_converted_final.png       1     1         False\n",
       "51     00318_940422_fa_converted_final.png       0     3         False\n",
       "121    00183_940128_fa_converted_final.png       0     1         False\n",
       "109  00164_931230_fa_a_converted_final.png       0     2         False\n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8224e50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:40.478949250Z",
     "start_time": "2024-01-13T18:45:40.466194421Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_df = pd.read_csv(\"../feret_generated.csv\")\n",
    "train_df = pd.concat([train_df, generated_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88601032-b66d-4fe5-abc0-b0ddb2bd5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>is_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00956_960627_fa_converted_final.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00746_941205_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00841_940307_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00475_940519_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00947_960627_fa_converted_final.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0_1_20240125145149259875.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>0_1_20240125145207210759.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0_1_20240125145227343355.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>0_1_20240125145247959868.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0_1_20240125145332628816.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>912 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  gender  race is_generated\n",
       "0    00956_960627_fa_converted_final.png       1     1          NaN\n",
       "1    00746_941205_fa_converted_final.png       0     1          NaN\n",
       "2    00841_940307_fa_converted_final.png       0     2          NaN\n",
       "3    00475_940519_fa_converted_final.png       0     0          NaN\n",
       "4    00947_960627_fa_converted_final.png       0     0          NaN\n",
       "..                                   ...     ...   ...          ...\n",
       "907         0_1_20240125145149259875.png       0     1         True\n",
       "908         0_1_20240125145207210759.png       0     1         True\n",
       "909         0_1_20240125145227343355.png       0     1         True\n",
       "910         0_1_20240125145247959868.png       0     1         True\n",
       "911         0_1_20240125145332628816.png       0     1         True\n",
       "\n",
       "[912 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3859e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:40.998183231Z",
     "start_time": "2024-01-13T18:45:40.982158705Z"
    }
   },
   "outputs": [],
   "source": [
    "root_path = \"../colored/\"\n",
    "generation_path = \"../accepted/\"\n",
    "train_df['image_path'] = train_df['filename'].apply(lambda x: os.path.join(root_path if \"final\" in x else generation_path, x))\n",
    "test_df['image_path'] = test_df['filename'].apply(lambda x: os.path.join(root_path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c370910-583d-4914-af73-72f531ce65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_four_test = test_df[test_df['race'] == 4]\n",
    "race_three_test = test_df[test_df['race'] == 3]\n",
    "race_two_test = test_df[test_df['race'] == 2]\n",
    "race_one_test = test_df[test_df['race'] == 1]\n",
    "race_zero_test = test_df[test_df['race'] == 0]\n",
    "\n",
    "chosen_race = race_two_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3fb791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:41.411571801Z",
     "start_time": "2024-01-13T18:45:41.400879425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../colored/00055_931230_fa_a_converted_final.png',\n",
       " '../colored/00088_931230_fa_a_converted_final.png',\n",
       " '../colored/00046_931230_fa_converted_final.png',\n",
       " '../colored/00150_931230_fa_a_converted_final.png',\n",
       " '../colored/00141_931230_fa_converted_final.png',\n",
       " '../colored/00153_931230_fa_converted_final.png',\n",
       " '../colored/00135_931230_fa_a_converted_final.png',\n",
       " '../colored/00128_931230_fa_a_converted_final.png',\n",
       " '../colored/00097_931230_fa_converted_final.png',\n",
       " '../colored/00044_931230_fa_converted_final.png',\n",
       " '../colored/00163_931230_fa_converted_final.png',\n",
       " '../colored/00110_931230_fa_converted_final.png',\n",
       " '../colored/00060_931230_fa_converted_final.png',\n",
       " '../colored/00100_931230_fa_a_converted_final.png',\n",
       " '../colored/00095_931230_fa_converted_final.png',\n",
       " '../colored/00160_931230_fa_a_converted_final.png',\n",
       " '../colored/00166_931230_fa_converted_final.png',\n",
       " '../colored/00133_931230_fa_converted_final.png',\n",
       " '../colored/00106_931230_fa_a_converted_final.png',\n",
       " '../colored/00099_931230_fa_a_converted_final.png',\n",
       " '../colored/00061_931230_fa_converted_final.png',\n",
       " '../colored/00156_931230_fa_a_converted_final.png',\n",
       " '../colored/00157_931230_fa_a_converted_final.png',\n",
       " '../colored/00167_931230_fa_converted_final.png',\n",
       " '../colored/00045_931230_fa_a_converted_final.png',\n",
       " '../colored/00054_931230_fa_a_converted_final.png',\n",
       " '../colored/00159_931230_fa_converted_final.png',\n",
       " '../colored/00117_931230_fa_a_converted_final.png',\n",
       " '../colored/00134_931230_fa_a_converted_final.png',\n",
       " '../colored/00058_931230_fa_converted_final.png',\n",
       " '../colored/00047_931230_fa_converted_final.png',\n",
       " '../colored/00147_931230_fa_converted_final.png',\n",
       " '../colored/00116_931230_fa_converted_final.png',\n",
       " '../colored/00029_960620_fa_converted_final.png',\n",
       " '../colored/00130_931230_fa_converted_final.png',\n",
       " '../colored/00020_931230_fa_converted_final.png',\n",
       " '../colored/00165_931230_fa_a_converted_final.png',\n",
       " '../colored/00164_931230_fa_a_converted_final.png']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths = train_df['image_path'].tolist()\n",
    "test_image_paths = chosen_race['image_path'].tolist()\n",
    "test_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7fd1f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:41.873848529Z",
     "start_time": "2024-01-13T18:45:41.869415976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read and resize images\n",
    "def preprocess_image(file_path, target_size=(512, 768)):\n",
    "    img = cv2.imread(file_path) \n",
    "    img = cv2.resize(img, (512,512))\n",
    "    x, y = 85, 0\n",
    "    crop_img = img[y:y+512, x:x+342]\n",
    "    resized_img = cv2.resize(crop_img, target_size)\n",
    "    final_img = resized_img.astype(\"float\") / 255.0  # Normalize pixel values\n",
    "    # cv2.imshow(\"test\", resized_img)\n",
    "    # cv2.waitKey(0)\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "088f404f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:50.810891980Z",
     "start_time": "2024-01-13T18:45:42.431807524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire dataset\n",
    "X_train = [preprocess_image(file_path) for file_path in train_image_paths]\n",
    "X_test = [preprocess_image(file_path) for file_path in test_image_paths]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8cec66",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-13T18:45:50.812277839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 768, 512, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3fb03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:50.818315320Z",
     "start_time": "2024-01-13T18:45:50.816808491Z"
    }
   },
   "outputs": [],
   "source": [
    "train_gender_labels = train_df['gender'].tolist()\n",
    "test_gender_labels = test_df['gender'].tolist()\n",
    "train_race_labels = train_df['race'].tolist()\n",
    "test_race_labels = chosen_race['race'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab0f5052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:50.835873913Z",
     "start_time": "2024-01-13T18:45:50.821176787Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create label encoders for gender and race\n",
    "gender_encoder = LabelEncoder()\n",
    "race_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to gender\n",
    "y_train_gender_encoded = gender_encoder.fit_transform(train_gender_labels)\n",
    "y_test_gender_encoded = gender_encoder.transform(test_gender_labels)\n",
    "\n",
    "# Apply label encoding to race\n",
    "y_train_race_encoded = race_encoder.fit_transform(train_race_labels)\n",
    "y_test_race_encoded = race_encoder.transform(test_race_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5a15e56-ca31-46d0-a56f-9a6e15aff994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.layers import Input, Dense, BatchNormalization, Conv2D, MaxPool2D, GlobalMaxPool2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "\n",
    "input_shape = (768,512, 3)\n",
    "num_classes_race = 5 \n",
    "\n",
    "def conv_block(inp, filters=32, bn=True, pool=True):\n",
    "    _ = Conv2D(filters=filters, kernel_size=3, activation='relu')(inp)\n",
    "    if bn:\n",
    "        _ = BatchNormalization()(_)\n",
    "    if pool:\n",
    "        _ = MaxPool2D()(_)\n",
    "    return _\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "_ = conv_block(input_layer, filters=32, bn=False, pool=False)\n",
    "_ = conv_block(_, filters=32*2)\n",
    "_ = conv_block(_, filters=32*3)\n",
    "_ = conv_block(_, filters=32*4)\n",
    "_ = conv_block(_, filters=32*5)\n",
    "_ = conv_block(_, filters=32*6)\n",
    "bottleneck = GlobalMaxPool2D()(_)\n",
    "\n",
    "# for race prediction\n",
    "_ = Dense(units=128, activation='relu')(bottleneck)\n",
    "race_output = Dense(units=num_classes_race, activation='softmax', name='race_output')(_)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[race_output])\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss={'race_output': 'categorical_crossentropy'},\n",
    "              loss_weights={'race_output': 1.5, },\n",
    "              metrics={'race_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24705812-4b4a-4ffb-99ff-4f558aa2a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 64\n",
    "valid_batch_size = 64\n",
    "train_gen = get_data_generator(df, train_idx, for_training=True, batch_size=batch_size)\n",
    "valid_gen = get_data_generator(df, valid_idx, for_training=True, batch_size=valid_batch_size)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=len(train_idx)//batch_size,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05f6d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:53.223267438Z",
     "start_time": "2024-01-13T18:45:50.826982783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 17:27:10.946793: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 17:27:11.622863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6719cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:45:53.593155508Z",
     "start_time": "2024-01-13T18:45:53.225545651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming RGB images with dimensions 100x150 pixels\n",
    "input_shape = (768,512, 3)\n",
    "num_classes_gender = 2 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes_gender, activation='softmax'))  # For gender classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd7aa5a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:53:36.795191221Z",
     "start_time": "2024-01-13T18:45:53.595209527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - ETA: 0s - loss: 11.6566 - accuracy: 0.6305"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 38\n  y sizes: 151\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example training code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_gender_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_gender_encoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FairnessLens/PreProcessor/venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/FairnessLens/PreProcessor/venv/lib/python3.8/site-packages/keras/src/engine/data_adapter.py:1950\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1943\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1944\u001b[0m         label,\n\u001b[1;32m   1945\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1946\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1947\u001b[0m         ),\n\u001b[1;32m   1948\u001b[0m     )\n\u001b[1;32m   1949\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1950\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 38\n  y sizes: 151\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Example training code\n",
    "model.fit(X_train, y_train_gender_encoded, epochs=10, batch_size=16, validation_data=(X_test, y_test_gender_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf41508c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T04:24:48.259880652Z",
     "start_time": "2023-12-28T04:24:47.079991745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 87ms/step - loss: 0.6342 - accuracy: 0.8411\n",
      "Test Accuracy: 0.8410596251487732\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_gender_encoded)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78e576c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T04:24:53.790199512Z",
     "start_time": "2023-12-28T04:24:53.532834384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 17:27:43.222524: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "input_shape = (768,512, 3)\n",
    "num_classes_race = 5 \n",
    "\n",
    "# race_model = Sequential()\n",
    "# race_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "# race_model.add(MaxPooling2D((2, 2)))\n",
    "# race_model.add(Flatten())\n",
    "# race_model.add(Dense(128, activation='relu'))\n",
    "# race_model.add(Dense(num_classes_race, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "input_layer = keras.Input(shape=input_shape, name=\"Input image\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(input_layer)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "out_a = keras.layers.Dense(1, activation='sigmoid', name='g_clf')(x)\n",
    "out_b = keras.layers.Dense(1, activation='linear', name='a_reg')(x)\n",
    " \n",
    "model = keras.Model( inputs = input_layer, outputs = [out_a, out_b], name=\"age_gender_model\")\n",
    "\n",
    "race_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5510a049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T04:32:45.222262449Z",
     "start_time": "2023-12-28T04:25:00.855870436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 17:27:56.474483: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4303355904 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 100s 2s/step - loss: 60.4733 - accuracy: 0.5110 - val_loss: 4.0435 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "57/57 [==============================] - 100s 2s/step - loss: 0.9631 - accuracy: 0.6886 - val_loss: 4.4184 - val_accuracy: 0.0789\n",
      "Epoch 3/5\n",
      "57/57 [==============================] - 99s 2s/step - loss: 0.5835 - accuracy: 0.8015 - val_loss: 6.9892 - val_accuracy: 0.0789\n",
      "Epoch 4/5\n",
      "57/57 [==============================] - 100s 2s/step - loss: 0.3076 - accuracy: 0.9035 - val_loss: 5.6808 - val_accuracy: 0.1053\n",
      "Epoch 5/5\n",
      "57/57 [==============================] - 99s 2s/step - loss: 0.1191 - accuracy: 0.9770 - val_loss: 9.4427 - val_accuracy: 0.0526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbdc030d130>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_model.fit(X_train, y_train_race_encoded, epochs=5, batch_size=16, validation_data=(X_test, y_test_race_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4d7691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T04:32:52.210265751Z",
     "start_time": "2023-12-28T04:32:51.125694937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 127ms/step - loss: 9.4427 - accuracy: 0.0526\n",
      "Test Accuracy: 0.05263157933950424\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = race_model.evaluate(X_test, y_test_race_encoded)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d827e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>yob</th>\n",
       "      <th>race</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>00738_941201_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>00500_940519_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1944</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>00733_941201_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954</td>\n",
       "      <td>Asian-Middle-Eastern</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00053_931230_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1963</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>00530_940519_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1954</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>00107_941121_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1953</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>00143_931230_fa_a_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1963</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>00315_940422_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1974</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>00487_940519_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1954</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>00139_931230_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973</td>\n",
       "      <td>Asian-Middle-Eastern</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>544 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  filename  gender   yob  \\\n",
       "678    00738_941201_fa_converted_final.png    Male  1954   \n",
       "448    00500_940519_fa_converted_final.png  Female  1944   \n",
       "673    00733_941201_fa_converted_final.png    Male  1954   \n",
       "18     00053_931230_fa_converted_final.png  Female  1963   \n",
       "478    00530_940519_fa_converted_final.png  Female  1954   \n",
       "..                                     ...     ...   ...   \n",
       "71     00107_941121_fa_converted_final.png  Female  1953   \n",
       "106  00143_931230_fa_a_converted_final.png    Male  1963   \n",
       "270    00315_940422_fa_converted_final.png  Female  1974   \n",
       "435    00487_940519_fa_converted_final.png  Female  1954   \n",
       "102    00139_931230_fa_converted_final.png    Male  1973   \n",
       "\n",
       "                     race                                         image_path  \n",
       "678                 White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "448                 White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "673  Asian-Middle-Eastern  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "18                  White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "478                 White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "..                    ...                                                ...  \n",
       "71                  White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "106                 White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "270                 White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "435                 White  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "102  Asian-Middle-Eastern  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "\n",
       "[544 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb984f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>yob</th>\n",
       "      <th>race</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>00707_941205_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1954</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>00663_941121_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1974</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>00099_931230_fa_a_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1963</td>\n",
       "      <td>Asian</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>00364_940422_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1974</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>00138_931230_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>00147_931230_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1963</td>\n",
       "      <td>Asian</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>00118_931230_fa_a_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1953</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>00086_931230_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1963</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>00480_940519_fa_converted_final.png</td>\n",
       "      <td>Female</td>\n",
       "      <td>1954</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>00623_940928_fa_converted_final.png</td>\n",
       "      <td>Male</td>\n",
       "      <td>1974</td>\n",
       "      <td>White</td>\n",
       "      <td>/home/mahdi/Projects/FairnessLens/FERET/colorf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  filename  gender   yob   race  \\\n",
       "647    00707_941205_fa_converted_final.png    Male  1954  White   \n",
       "607    00663_941121_fa_converted_final.png  Female  1974  White   \n",
       "63   00099_931230_fa_a_converted_final.png    Male  1963  Asian   \n",
       "319    00364_940422_fa_converted_final.png    Male  1974  White   \n",
       "101    00138_931230_fa_converted_final.png    Male  1973  White   \n",
       "..                                     ...     ...   ...    ...   \n",
       "110    00147_931230_fa_converted_final.png  Female  1963  Asian   \n",
       "82   00118_931230_fa_a_converted_final.png    Male  1953  White   \n",
       "51     00086_931230_fa_converted_final.png    Male  1963  White   \n",
       "428    00480_940519_fa_converted_final.png  Female  1954  White   \n",
       "569    00623_940928_fa_converted_final.png    Male  1974  White   \n",
       "\n",
       "                                            image_path  \n",
       "647  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "607  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "63   /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "319  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "101  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "..                                                 ...  \n",
       "110  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "82   /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "51   /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "428  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "569  /home/mahdi/Projects/FairnessLens/FERET/colorf...  \n",
       "\n",
       "[136 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226076da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
